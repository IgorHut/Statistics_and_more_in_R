---
title: "Correlation tests and correlation matrix in R"
author: "Igor Hut"
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    toc: yes
    toc_depth: 3
  html_notebook: default
---

**The following content is mostly based on the material that can be found at <http://www.sthda.com/>, as well as in the vignette for `corrplot` R package - [An Introduction to corrplot Package](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html)**


### Install and load required R packages

We'll use the `ggpubr` R package for an easy ggplot2-based data visualization and `corrplot` package to plot correlograms:
```{r, message=FALSE, warning=FALSE}
library(ggpubr)
```

## Methods for correlation analyses

There are different methods to perform correlation analysis:

- **Pearson correlation (r)**, which measures a linear dependence between two variables (x and y). It's also known as a parametric correlation test because it depends to the distribution of the data. It can be used only when x and y are from normal distribution. The plot of *y = f(x)* is named the *linear regression curve*.

- **Kendall $\tau$** and **Spearman $\rho$**, which are rank-based correlation coefficients (non-parametric)

- **The most commonly used method is the Pearson correlation method**

## Compute correlation in R

### R functions

Correlation coefficients can be computed in R by using the functions `cor()` and `cor.test()`:

> 
- `cor()` computes the correlation coefficient
- `cor.test()` test for association/correlation between paired samples. It returns both the correlation coefficient and the significance level(or p-value) of the correlation.

The simplified formats are:
```{r, eval=FALSE, message=FALSE}
cor(x, y, method = c("pearson", "kendall", "spearman"))
cor.test(x, y, method=c("pearson", "kendall", "spearman"))
```

where:

> 
- x, y: numeric vectors with the same length
- method: correlation method


**If the data contain missing values, the following R code can be used to handle missing values by case-wise deletion:**
```{r, eval=FALSE}
cor(x, y,  method = "pearson", use = "complete.obs")
```

## Preliminary considerations 

We'll use the well known built-in `mtcars` R dataset.
```{r}
head(mtcars)
```

> We'd like to compute the correlation between `mpg` and `wt` variables.

**First let's visualise our data by the means of a scatter plot. We'll be using `ggpubr` R package**

```{r}
library(ggpubr)

my_data <- mtcars
my_data$cyl <- factor(my_data$cyl)
str(my_data)

ggscatter(my_data, x = "wt", y = "mpg",
          add = "reg.line", conf.int = TRUE,
          cor.coef = TRUE, cor.method = "pearson",
          xlab = "Weight (1000 lbs)", ylab = "Miles/ (US) gallon")
```

### Preleminary test to check the test assumptions

1. Is the relation between variables linear? Yes, from the plot above, the relationship can be, closely enough, modeled as linear. In the situation where the scatter plots show curved patterns, we are dealing with nonlinear association between the two variables.

2. Are the data from each of the 2 variables (x, y) following a normal distribution?
    - Use *Shapiro-Wilk* normality test -> R function: `shapiro.test()` 
    - and look at the normality plot -> R function: `ggpubr::ggqqplot()`

- *Shapiro-Wilk* test can be performed as follow:
    - Null hypothesis: the data are normally distributed
    - Alternative hypothesis: the data are not normally distributed
    
```{r}
# Shapiro-Wilk normality test for mpg
shapiro.test(my_data$mpg) # => p = 0.1229

# Shapiro-Wilk normality test for wt
shapiro.test(my_data$wt) # => p = 0.09
```
*As can be seen from the output, the two p-values are greater than the predetermined significance level of 0.05 implying that the distribution of the data are not significantly different from normal distribution. In other words, we can assume the normality.*

- One more option for checking the normality of the data distribution is visual inspection of the Q-Q plots (quantile-quantile plots). Q-Q plot draws the correlation between a given sample and the theoretical normal distribution.

Again, we'll use the `ggpubr` R package to obtain "pretty", i.e. publishing-ready, Q-Q plots.

```{r}
library("ggpubr")
# Check for the normality of "mpg""
ggqqplot(my_data$mpg, ylab = "MPG")

# Check for the normality of "wt""
ggqqplot(my_data$wt, ylab = "WT")
```

*From the Q-Q normality plots, we can assume that both samples may come from populations that, closely enough, follow normal distributions.*

**It is important to note that if the data does not follow the normal distribution, at least closely enough, it's recommended to use the non-parametric correlation, including Spearman and Kendall rank-based correlation tests.**

## Pearson correlation test

**Example:**

```{r}
res <- cor.test(my_data$wt, my_data$mpg, method = "pearson")
res
```

So what's happening here? First of all let's clarify the meaning of this printout:

- `t` is the *t-test* statistic value `(t = -9.559)`,
- `df` is the degrees of freedom `(df= 30)`,
- `p-value` is the significance level of the *t-test* `(p-value =` $1.29410^{-10}$`)`.
- `conf.int` is the *confidence interval* of the correlation coefficient at 95% `(conf.int = [-0.9338, -0.7441])`;
- `sample estimates` is the *correlation coefficient* `(Cor.coeff = -0.87)`.

**Interpretation of the results:**
As can be see from the results above the *p-value* of the test is $1.29410^{-10}$, which is less than the significance level $\alpha = 0.05$. We can conclude that `wt` and `mpg` are significantly correlated with a correlation coefficient of -0.87 and *p-value* of $1.29410^{-10}$.

**Access to the values returned by `cor.test()` function**

The function `cor.test()` returns a list containing the following components:

```{r}
str(res)
```
Of these we are most interested with:

- `p.value`: the p-value of the test
- `estimate`: the correlation coefficient

```{r}
# Extract the p.value
res$p.value

# Extract the correlation coefficient
res$estimate
```

## Kendall rank correlation test

**The Kendall rank correlation coefficient** or **Kendall's $\tau$ statistic** is used to estimate a rank-based measure of association. This test may be used if the data do not necessarily come from a bivariate normal distribution.

```{r}
res2 <- cor.test(my_data$mpg, my_data$wt, method = "kendall")

res2
```

Here `tau` is the Kendall correlation coefficient, so The correlation coefficient between `mpg` and `wy` is -0.7278 and the p-value is $6.70610^{-9}$.

## Spearman rank correlation coefficient

**Spearman's $\rho$ statistic** is also used to estimate a rank-based measure of association. This test may be used if the data do not come from a bivariate normal distribution.

```{r}
res3 <- cor.test(my_data$wt, my_data$mpg, method = "spearman")

res3
```
Here, `rho` is the Spearman's correlation coefficient, so the correlation coefficient between `mpg` and `wt` is -0.8864 and the p-value is $1.48810^{-11}$.

## How to interpret correlation coefficient

**Value of the correlation coefficient can vary between -1 and 1:**

- -1 indicates a strong negative correlation : this means that every time x increases, y decreases 
- 0 means that there is no association between the two variables (x and y) 
- 1 indicates a strong positive correlation : this means that y increases with x 


## What is a correlation matrix?

Previously, we described how to perform correlation test between two variables. In the following sections we'll see how a **correlation matrix** can be computed and visualized. The **correlation matrix** is used to investigate the dependence between multiple variables at the same time. The result is a table containing the correlation coefficients between each variable and the others.

## Compute correlation matrix in R

We have already mentioned the `cor()` function, at the intoductory part of this document dealing with the correlation test for a bivariate case. It be used to compute a correlation matrix. A simplified format of the function is :

```{r, eval=FALSE, message=FALSE}
cor(x, method = c("pearson", "kendall", "spearman")) 
```
Here:

- `x` is numeric matrix or a data frame.
- `method`: indicates the correlation coefficient to be computed. The default is "pearson"" correlation coefficient which measures the linear dependence between two variables. As already explained "kendall" and "spearman" correlation methods are non-parametric rank-based correlation tests.

If your data contain missing values, the following R code can be used to handle missing values by case-wise deletion:

```{r, eval=FALSE, message=FALSE}
cor(x, method = "pearson", use = "complete.obs")
```

